{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1a0de6ac11c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "from numpy import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load('classes_sherlock.npy', allow_pickle=True)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_columns = ['address', 'age', 'area', 'birth Date', 'birth Place', 'brand', 'city', 'continent', 'country', 'county', 'currency', 'day', 'duration', 'industry', 'language', 'location', 'manufacturer', 'name', 'nationality', 'order', 'person', 'product', 'range', 'rank', 'region','sales', 'sex', 'state', 'status', 'symbol', 'type', 'year']\n",
    "acceptable_columns = np.array(acceptable_columns)\n",
    "# np.save(\"classes_sherlock.npy\", acceptable_columns, allow_pickle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main dataframes to add onto existing dataframe\n",
    "new_train = pd.DataFrame(columns = ['type', 'values'])\n",
    "new_test = pd.DataFrame(columns = ['type', 'values'])\n",
    "new_val = pd.DataFrame(columns = ['type', 'values'])\n",
    "\n",
    "# list of urls of csvs\n",
    "urls = ['https://drive.google.com/file/d/1VBEYn2bV9hUg6Vs97StMMR8NudP--v-D/view?usp=sharing', 'https://drive.google.com/file/d/1XoHPXvb_eAH-90jTSj-1sePVzQtIlw-n/view?usp=sharing',\n",
    "       'https://drive.google.com/file/d/1QwjBOKHSYbEqZxDeUfVZVOjkgb_xuaUt/view?usp=sharing', 'https://drive.google.com/file/d/1I4btyUubuYR6sgz2YhfEftNXdJuKaPaN/view?usp=sharing',\n",
    "       'https://drive.google.com/file/d/1X6B-9hRgENR4xAjL1d0m6BmxGZUs6eGF/view?usp=sharing', 'https://drive.google.com/file/d/1DiHNQZUb9icUhkeBvWYyARKYscDPUAyx/view?usp=sharing',\n",
    "       'https://drive.google.com/file/d/1GhY41caqsjYhTVdUaHqyzp6b-bUJSiX9/view?usp=sharing', 'https://drive.google.com/file/d/1ISLN-fra-7rzfar4b8LvM20YF_UNWC52/view?usp=sharing',\n",
    "       'https://drive.google.com/file/d/140MKfvrMnIy-sfNlovGmHShEETBagMBw/view?usp=sharing']\n",
    "# list of types\n",
    "types = ['naic', 'credit card account numbers', 'credit card account numbers', 'credit card account numbers', 'fips code', 'lei',\n",
    "        'mcc code', 'phone number', 'zip code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(urls)):\n",
    "    url = urls[i]\n",
    "    file_id = url.split('/')[-2]\n",
    "    dwn_url='https://drive.google.com/uc?id=' + file_id\n",
    "    data = pd.read_csv(dwn_url, sep=\";\", header=None)\n",
    "    t = types[i]\n",
    "    \n",
    "    # split 0.8 to training, 0.1 to test, and 0.1 to validation\n",
    "    train_idx = round(len(data)*0.8)\n",
    "    test_idx = round(len(data)*0.15)\n",
    "    train_data = data.iloc[:train_idx]\n",
    "    test_data = data.iloc[train_idx: (train_idx+test_idx)]\n",
    "    val_data = data.iloc[(train_idx+test_idx):]\n",
    "\n",
    "    # splitting into chunks of ~five data points\n",
    "    train_data = train_data.values.tolist()\n",
    "    train_data = list(itertools.chain(*train_data))\n",
    "    splitval = train_idx/5\n",
    "    train_data = [str(i) for i in train_data]\n",
    "    train_data_split = np.array_split(train_data, splitval)\n",
    "    train_df = pd.Series(train_data_split, name='values').to_frame()\n",
    "    train_df['type'] = t\n",
    "    train_df = train_df.iloc[:,[1,0]]\n",
    "    train_df['values'] = [str(i) for i in train_df['values']]\n",
    "\n",
    "    splitval = round(test_idx/5)\n",
    "    test_data = test_data.values.tolist()\n",
    "    test_data = list(itertools.chain(*test_data))\n",
    "    test_data = [str(i) for i in test_data]\n",
    "    test_data_split = np.array_split(test_data, splitval)\n",
    "    test_df = pd.Series(test_data_split, name='values').to_frame()\n",
    "    test_df['type'] = t\n",
    "    test_df = test_df.iloc[:,[1,0]]\n",
    "    test_df['values'] = [str(i) for i in test_df['values']]\n",
    "\n",
    "    splitval = round(len(val_data)/5)\n",
    "    val_data = val_data.values.tolist()\n",
    "    val_data = list(itertools.chain(*val_data))\n",
    "    val_data = [str(i) for i in val_data]\n",
    "    val_data_split = np.array_split(val_data, splitval)\n",
    "    val_df = pd.Series(val_data_split, name='values').to_frame()\n",
    "    val_df['type'] = t\n",
    "    val_df = val_df.iloc[:,[1,0]] \n",
    "    val_df['values'] = [str(i) for i in val_df['values']]\n",
    "    \n",
    "    # add to main dataframes\n",
    "    new_train = new_train.append(train_df, ignore_index=True)\n",
    "    new_test = new_test.append(test_df, ignore_index=True)\n",
    "    new_val = new_val.append(val_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naic</td>\n",
       "      <td>[713910, 71392, 713920, 71393, 713930]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>naic</td>\n",
       "      <td>[71394, 713940, 71395, 713950, 71399]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naic</td>\n",
       "      <td>[713990, 72, 721, 7211, 72111]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>naic</td>\n",
       "      <td>[721110, 72112, 721120, 72119, 721191]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>naic</td>\n",
       "      <td>[721199, 7212, 72121, 721211, 721214]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>zip code</td>\n",
       "      <td>[917, 641, 612, 611, 687]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1845</th>\n",
       "      <td>zip code</td>\n",
       "      <td>[646, 692, 692, 674, 687]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>zip code</td>\n",
       "      <td>[693, 694, 765, 766, 720]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1847</th>\n",
       "      <td>zip code</td>\n",
       "      <td>[767, 653, 606, 698, 850]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1848</th>\n",
       "      <td>zip code</td>\n",
       "      <td>[840, 820, 830, 802]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1849 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                                  values\n",
       "0         naic  [713910, 71392, 713920, 71393, 713930]\n",
       "1         naic   [71394, 713940, 71395, 713950, 71399]\n",
       "2         naic          [713990, 72, 721, 7211, 72111]\n",
       "3         naic  [721110, 72112, 721120, 72119, 721191]\n",
       "4         naic   [721199, 7212, 72121, 721211, 721214]\n",
       "...        ...                                     ...\n",
       "1844  zip code               [917, 641, 612, 611, 687]\n",
       "1845  zip code               [646, 692, 692, 674, 687]\n",
       "1846  zip code               [693, 694, 765, 766, 720]\n",
       "1847  zip code               [767, 653, 606, 698, 850]\n",
       "1848  zip code                    [840, 820, 830, 802]\n",
       "\n",
       "[1849 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_values = pq.read_table(source=\"./data/raw/train_values.parquet\").to_pandas()\n",
    "train_df_labels = pq.read_table(source=\"./data/raw/train_labels.parquet\").to_pandas()\n",
    "train_df_full = train_df_labels.merge(train_df_values, left_index=True, right_index=True)\n",
    "train_df_full = train_df_full[train_df_full['type'].isin(acceptable_columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc130680",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_train = pd.DataFrame(columns = ['type', 'values'])\n",
    "old_test = pd.DataFrame(columns = ['type', 'values'])\n",
    "old_val = pd.DataFrame(columns = ['type', 'values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85efdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each semantic type in original Sherlock data into 0.8 for training, 0.15 for testing, 0.05 for validation\n",
    "for i in range(len(acceptable_columns)):\n",
    "    temp_df = train_df_full[train_df_full[\"type\"] == acceptable_columns[i]]\n",
    "    training_idx = round(len(temp_df)*0.8)\n",
    "    testing_idx = round(len(temp_df)*0.15)\n",
    "    training_data = temp_df.iloc[:training_idx]\n",
    "    testing_data = temp_df.iloc[training_idx: (training_idx+testing_idx)]\n",
    "    valid_data = temp_df.iloc[(training_idx+testing_idx):]\n",
    "    \n",
    "    old_train = old_train.append(training_data, ignore_index=True)\n",
    "    old_test = old_test.append(testing_data, ignore_index=True)\n",
    "    old_val = old_val.append(valid_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df_full = old_test.append(new_test, ignore_index=True)\n",
    "test_df_values = pd.DataFrame(test_df_full, columns=['values'])\n",
    "test_df_labels = pd.DataFrame(test_df_full, columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_full = old_train.append(new_train, ignore_index=True)\n",
    "train_df_values = pd.DataFrame(train_df_full, columns=['values'])\n",
    "train_df_labels = pd.DataFrame(train_df_full, columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_full = old_val.append(new_val, ignore_index=True)\n",
    "val_df_values = pd.DataFrame(val_df_full, columns=['values'])\n",
    "val_df_labels = pd.DataFrame(val_df_full, columns=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_labels.to_parquet(\"./data/raw/test_labels.parquet\", engine='fastparquet')\n",
    "test_df_values.to_parquet(\"./data/raw/test_values.parquet\", engine='fastparquet')\n",
    "\n",
    "train_df_labels.to_parquet(\"./data/raw/train_labels.parquet\", engine='fastparquet')\n",
    "train_df_values.to_parquet(\"./data/raw/train_values.parquet\", engine='fastparquet')\n",
    "\n",
    "val_df_labels.to_parquet(\"./data/raw/val_labels.parquet\", engine='fastparquet')\n",
    "val_df_values.to_parquet(\"./data/raw/val_values.parquet\", engine='fastparquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO:\n",
    "# add a try catch for putting in an unvalid link\n",
    "# also maybe a try catch for if the data is not in the right format (i.e. not in a csv with a single column)\n",
    "# get rid of any unnecessary code\n",
    "# hide warnings so the warning about append won't show up or change the code to use concat instead\n",
    "# add any extra comments to make the code more readable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('myenv3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "93982b2d13d9986a928bb816b0e17d444852884eb3b4b4fc01736d695f21d025"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
