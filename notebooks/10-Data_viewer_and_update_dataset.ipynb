{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "027837a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import load\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import requests\n",
    "from io import StringIO\n",
    "import pyarrow.parquet as pq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbeac41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = load('classes_sherlock.npy', allow_pickle=True)\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc560033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceptable_columns = ['address', 'age', 'area', 'birth Date', 'birth Place', 'brand', 'city', 'continent', 'country', 'county', 'currency', 'day', 'duration', 'industry', 'language', 'location', 'manufacturer', 'name', 'nationality', 'order', 'person', 'product', 'range', 'rank', 'region','sales', 'sex', 'state', 'status', 'symbol', 'type', 'year']\n",
    "# acceptable_columns = np.array(acceptable_columns)\n",
    "# np.save(\"classes_sherlock.npy\", acceptable_columns, allow_pickle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdd0afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # main dataframes to add onto existing dataframe\n",
    "# new_train = pd.DataFrame(columns = ['type', 'values'])\n",
    "# new_test = pd.DataFrame(columns = ['type', 'values'])\n",
    "# new_val = pd.DataFrame(columns = ['type', 'values'])\n",
    "\n",
    "# # list of urls of csvs\n",
    "# urls = ['https://drive.google.com/file/d/1VBEYn2bV9hUg6Vs97StMMR8NudP--v-D/view?usp=sharing', 'https://drive.google.com/file/d/1XoHPXvb_eAH-90jTSj-1sePVzQtIlw-n/view?usp=sharing',\n",
    "#        'https://drive.google.com/file/d/1QwjBOKHSYbEqZxDeUfVZVOjkgb_xuaUt/view?usp=sharing', 'https://drive.google.com/file/d/1I4btyUubuYR6sgz2YhfEftNXdJuKaPaN/view?usp=sharing',\n",
    "#        'https://drive.google.com/file/d/1X6B-9hRgENR4xAjL1d0m6BmxGZUs6eGF/view?usp=sharing', 'https://drive.google.com/file/d/1DiHNQZUb9icUhkeBvWYyARKYscDPUAyx/view?usp=sharing',\n",
    "#        'https://drive.google.com/file/d/1GhY41caqsjYhTVdUaHqyzp6b-bUJSiX9/view?usp=sharing', 'https://drive.google.com/file/d/1ISLN-fra-7rzfar4b8LvM20YF_UNWC52/view?usp=sharing',\n",
    "#        'https://drive.google.com/file/d/140MKfvrMnIy-sfNlovGmHShEETBagMBw/view?usp=sharing']\n",
    "# # list of types\n",
    "# types = ['naic', 'credit card account numbers', 'credit card account numbers', 'credit card account numbers', 'fips code', 'lei',\n",
    "#         'mcc code', 'phone number', 'zip code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72708055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(urls)):\n",
    "#     url = urls[i]\n",
    "#     file_id = url.split('/')[-2]\n",
    "#     dwn_url='https://drive.google.com/uc?id=' + file_id\n",
    "#     data = pd.read_csv(dwn_url, sep=\";\", header=None)\n",
    "#     t = types[i]\n",
    "\n",
    "#     # split 0.8 to training, 0.1 to test, and 0.1 to validation\n",
    "#     train_idx = round(len(data)*0.8)\n",
    "#     test_idx = round(len(data)*0.1)\n",
    "#     train_data = data.iloc[:train_idx]\n",
    "#     test_data = data.iloc[train_idx: (train_idx+test_idx)]\n",
    "#     val_data = data.iloc[(train_idx+test_idx):]\n",
    "\n",
    "#     # splitting into chunks of ~five data points\n",
    "#     train_data = train_data.values.tolist()\n",
    "#     train_data = list(itertools.chain(*train_data))\n",
    "#     splitval = train_idx/5\n",
    "#     train_data_split = np.array_split(train_data, splitval)\n",
    "#     train_df = pd.Series(train_data_split, name='values').to_frame()\n",
    "#     train_df['type'] = t\n",
    "#     train_df = train_df.iloc[:,[1,0]]\n",
    "\n",
    "#     splitval = round(test_idx/5)\n",
    "#     test_data = test_data.values.tolist()\n",
    "#     test_data = list(itertools.chain(*test_data))\n",
    "#     test_data_split = np.array_split(test_data, splitval)\n",
    "#     test_df = pd.Series(test_data_split, name='values').to_frame()\n",
    "#     test_df['type'] = t\n",
    "#     test_df = test_df.iloc[:,[1,0]]\n",
    "\n",
    "#     splitval = round(len(val_data)/5)\n",
    "#     val_data = val_data.values.tolist()\n",
    "#     val_data = list(itertools.chain(*val_data))\n",
    "#     val_data_split = np.array_split(val_data, splitval)\n",
    "#     val_df = pd.Series(val_data_split, name='values').to_frame()\n",
    "#     val_df['type'] = t\n",
    "#     val_df = val_df.iloc[:,[1,0]] \n",
    "    \n",
    "#     # add to main dataframes\n",
    "#     new_train = new_train.append(train_df, ignore_index=True)\n",
    "#     new_test = new_test.append(test_df, ignore_index=True)\n",
    "#     new_val = new_val.append(val_df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1fa4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9b85b48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m test_df_labels \u001b[38;5;241m=\u001b[39m pq\u001b[38;5;241m.\u001b[39mread_table(source\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/data/raw/test_labels.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# test_df_full = test_df_labels.merge(test_df_values, left_index=True, right_index=True)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# test_df_full = test_df_full[test_df_full['type'].isin(acceptable_columns)]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# add new testing data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# test_df_full = test_df_full.append(new_test, ignore_index=True)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m test_df_values \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mtest_df_full\u001b[49m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      8\u001b[0m test_df_labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test_df_full, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df_full' is not defined"
     ]
    }
   ],
   "source": [
    "test_df_values = pq.read_table(source=\"../data/data/raw/test_values.parquet\").to_pandas()\n",
    "test_df_labels = pq.read_table(source=\"../data/data/raw/test_labels.parquet\").to_pandas()\n",
    "# test_df_full = test_df_labels.merge(test_df_values, left_index=True, right_index=True)\n",
    "# test_df_full = test_df_full[test_df_full['type'].isin(acceptable_columns)]\n",
    "# add new testing data\n",
    "# test_df_full = test_df_full.append(new_test, ignore_index=True)\n",
    "test_df_values = pd.DataFrame(test_df_full, columns=['values'])\n",
    "test_df_labels = pd.DataFrame(test_df_full, columns=['type'])\n",
    "# len(test_df_full.iloc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea1434",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_labels.loc[test_df_labels['type'] == \"phone number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d24ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_values = pq.read_table(source=\"./data/raw/train_values.parquet\").to_pandas()\n",
    "train_df_labels = pq.read_table(source=\"./data/raw/train_labels.parquet\").to_pandas()\n",
    "train_df_full = train_df_labels.merge(train_df_values, left_index=True, right_index=True)\n",
    "train_df_full = train_df_full[train_df_full['type'].isin(acceptable_columns)]\n",
    "# add new training data\n",
    "train_df_full = train_df_full.append(new_train, ignore_index=True)\n",
    "train_df_values = pd.DataFrame(train_df_full, columns=['values'])\n",
    "train_df_labels = pd.DataFrame(train_df_full, columns=['type'])\n",
    "train_df_full.loc[train_df_full[\"type\"] == \"symbol\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8fa565",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_values = pq.read_table(source=\"./data/raw/val_values.parquet\").to_pandas()\n",
    "val_df_labels = pq.read_table(source=\"./data/raw/val_labels.parquet\").to_pandas()\n",
    "val_df_full = val_df_labels.merge(val_df_values, left_index=True, right_index=True)\n",
    "val_df_full = val_df_full[val_df_full['type'].isin(acceptable_columns)]\n",
    "# add new validation data\n",
    "val_df_full = val_df_full.append(new_val, ignore_index=True)\n",
    "val_df_values = pd.DataFrame(val_df_full, columns=['values'])\n",
    "val_df_labels = pd.DataFrame(val_df_full, columns=['type'])\n",
    "val_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27fd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df_labels.to_parquet(\"./data/raw/test_labels.parquet\", engine='fastparquet')\n",
    "# test_df_values.to_parquet(\"./data/raw/test_values.parquet\", engine='fastparquet')\n",
    "\n",
    "# train_df_labels.to_parquet(\"./data/raw/train_labels.parquet\", engine='fastparquet')\n",
    "# train_df_values.to_parquet(\"./data/raw/train_values.parquet\", engine='fastparquet')\n",
    "\n",
    "# val_df_labels.to_parquet(\"./data/raw/val_labels.parquet\", engine='fastparquet')\n",
    "# val_df_values.to_parquet(\"./data/raw/val_values.parquet\", engine='fastparquet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040250c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "93982b2d13d9986a928bb816b0e17d444852884eb3b4b4fc01736d695f21d025"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
